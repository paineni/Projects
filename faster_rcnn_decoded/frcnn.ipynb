{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchinfo import summary\n",
    "from faster_rcnn_from_scratch.utils import image_visualization as iv\n",
    "from faster_rcnn_from_scratch.utils import prepare_dataset\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms import v2\n",
    "from typing import List\n",
    "from faster_rcnn_from_scratch.utils.anchors import generate_anchor_boxes\n",
    "from faster_rcnn_from_scratch.utils import iou\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(36)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading an image\n",
    "\n",
    "- We load a simple Pedestrian Dataset which consists of Pedestrains and Corresponding Masks\n",
    "- From the masks we generate bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image(\"PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "mask = read_image(\"PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
    "image_mask_pair = [image, mask]\n",
    "figsize = (10, 8)\n",
    "iv.plot(image_mask_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"PennFudanPed\"\n",
    "image_dir = \"PNGImages\"\n",
    "mask_dir = \"PedMasks\"\n",
    "ds = prepare_dataset.Masks2BboxDs(\n",
    "    root,\n",
    "    image_dir,\n",
    "    mask_dir,\n",
    "    transforms=v2.Resize(\n",
    "        (800, 800), interpolation=v2.functional.InterpolationMode.BICUBIC\n",
    "    ),\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = ds[0]\n",
    "masks = target[\"masks\"]\n",
    "gt_labels = target[\"labels\"]\n",
    "gt_bboxes = target[\"boxes\"]\n",
    "\n",
    "colors = [mcolors.rgb2hex(plt.get_cmap(\"tab10\")(i)) for i in range(len(masks))]\n",
    "font = \"UbuntuMono-R.ttf\"\n",
    "bbox_image = draw_bounding_boxes(\n",
    "    image,\n",
    "    gt_bboxes,\n",
    "    gt_labels,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=True,\n",
    "    font_size=30,\n",
    "    colors=colors,\n",
    ")\n",
    "iv.plot(bbox_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "- We would like to have features to generate anchors\n",
    "- To generate features we use VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.vgg16(weights=True).to(device)\n",
    "summary(model, (1, 3, 800, 800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collecting layers with output feature map size i.e both width and height of feature map are < 50 (50 is used in the paper)\n",
    "dummy_img = torch.zeros(\n",
    "    (1, 3, 800, 800)\n",
    ").float()  # test image array [1, 3, 800, 800]\n",
    "print(dummy_img.shape)\n",
    "\n",
    "req_features = []\n",
    "k = dummy_img.clone().to(device)\n",
    "for i in list(model.features[:30]):\n",
    "    k = i(k)\n",
    "    req_features.append(i)\n",
    "    out_channels = k.size()[1]\n",
    "print(len(req_features))  # 30 # Total features taken are\n",
    "print(out_channels)  # 512 # Depth of the layer's op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = nn.Sequential(*req_features).to(device)\n",
    "image_copy = image.float().to(device).unsqueeze(0)\n",
    "# Running feature extractor on our image\n",
    "feature_map = feature_extractor(image_copy)\n",
    "imgArray = feature_map.data.cpu().squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize few features\n",
    "num_plots = 10\n",
    "iv.plot(\n",
    "    [imgArray[i] for i in range(num_plots)],\n",
    "    subplots_kwargs={\"figsize\": (20, 20)},\n",
    "    subplot_adjust_kwargs={\"hspace\": 0, \"wspace\": 0.3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Anchor Bboxes\n",
    "- Once we have got the feature map, in our case of size (512,50,50)\n",
    "- We need Anchors, to generate Anchor Bboxes.\n",
    "- Anchors are nothing but every pixel of the single channeled feature map (So we have 2500 anchors)\n",
    "- These pixels are representation of a patch in the original image\n",
    "- Another interesting bit is since we used VGG16 from beginning to end of final conv stack of VGG, we got 50x50xC feature map, from 800x800x3 image.\n",
    "  - This conveys one pixel on the map corresponds to 16 by 16 patch on the original image\n",
    "  - So if you move pixel wise one by one on the feature map, one can imagine a patch of size 16x16 moving correspondingly on the image\n",
    "  - For this patch on image we will take it's center\n",
    "- Now to generate Anchor Boxes, we map the anchor points from feature map to the original image\n",
    "  - On the original image we generate various size bboxes\n",
    "  - Precisely, for each anchor at the **center** we generate 9 bboxes, each in 3 ratios (small, medium & large) and in 3 shapes (square(surrounding the anchor), 2 rectangles perpendicular to each other)\n",
    "- Bboxes are generated in following manner:\n",
    "  - Consider a 16 by 16 patch on original image, calculate it's center\n",
    "  - This center can be considered as a back-representation of anchor we see on the feature map\n",
    "  - On this center we generate 9 bboxes as mentioned above, thus for 2500 centres 9 bboxes leading to 22500 bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_size = 50  # Size of the feature map (example value)\n",
    "window_size = (\n",
    "    800 // feature_map_size\n",
    ")  # The stride of the sliding window or feature map stride\n",
    "aspect_ratios = [0.5, 1, 2]\n",
    "scales = [8, 16, 32]\n",
    "\n",
    "anchor_bboxes = generate_anchor_boxes(\n",
    "    feature_map_size, window_size, aspect_ratios, scales\n",
    ")\n",
    "print(anchor_bboxes.shape)  # (22500, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Visualize the anchor boxes on the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    anchor_bboxes, format=\"XYXY\", canvas_size=v2.functional.get_size(image)\n",
    ")\n",
    "\n",
    "colors = [\n",
    "    mcolors.rgb2hex(plt.get_cmap(\"tab20\")(i)) for i in range(len(torch_bboxes))\n",
    "]\n",
    "font = \"UbuntuMono-R.ttf\"\n",
    "bbox_image = draw_bounding_boxes(\n",
    "    image,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    colors=colors,\n",
    ")\n",
    "iv.plot(bbox_image, subplots_kwargs={\"figsize\": (5, 5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Validating the Anchor Bboxes\n",
    "\n",
    "- We remove the bboxes who go out of the condition i.e. xmin, ymin being < 0 and xmax,ymax being > 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = np.where(\n",
    "    (anchor_bboxes[:, 0] >= 0)\n",
    "    & (anchor_bboxes[:, 1] >= 0)\n",
    "    & (anchor_bboxes[:, 2] <= 800)\n",
    "    & (anchor_bboxes[:, 3] <= 800)\n",
    ")[0]\n",
    "valid_boxes = anchor_bboxes[valid_indices]\n",
    "print(type(valid_boxes), (valid_boxes.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "- Once we have generated all the anchor boxes, we need to look at the objects inside the image and assign them to the specific anchor boxes which contain them. Faster-R-CNN has some guidelines to assign labels to the anchor boxes\n",
    "- We assign a positive label(1 or can be any int >=1 depending on different object categories) to 2 kind of anchor boxes:\n",
    "  - All the anchor boxes with the highest IoU overlap with gt-box or\n",
    "  - The anchor box that has an IoU overlap higher than 0.7 with gt-box\n",
    "    - Basically finding out which gt has highest overlap with an anchor box\n",
    "    - Later verifying if that IoU is greater than 0.7 or not\n",
    "  - With these conditions a single gt may assign positive labels to multiple anchor boxes\n",
    "- We assign a negative label(0) to a anchor box if it's IoU ratio is lower than 0.3 for all gt-boxes\n",
    "- **Anchor boxes which are neither positive nor negative do not contribute to the training objective**\n",
    "- For training a RPN we use only 1 and 0 as labels where 1 represents there is an object and 0 means background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label array of length valid anchor boxes and fill it with -1\n",
    "valid_labels = np.empty((len(valid_boxes)), dtype=np.int8)\n",
    "valid_labels.fill(-1)\n",
    "\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_boxes_tensor = tv_tensors.BoundingBoxes(\n",
    "    valid_boxes, format=\"XYXY\", canvas_size=v2.functional.get_size(image)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each valid anchor box calculate the IoU with each gt object.\n",
    "# Since we have 8940 anchor boxes and 2 gt_objects, we should get an array of\n",
    "# (8940,2) where each collumn depicts whether anchor bbox is +ve or not\n",
    "\n",
    "# Complete box iou function takes cross product kind of calculation, while the\n",
    "# simple box_iou takes dot product kind of calculation, calculating iou's only in between corresponding bboxes in 2 arrays\n",
    "\n",
    "input_boxes = valid_boxes_tensor.data\n",
    "\n",
    "gt_boxes = gt_bboxes.data\n",
    "\n",
    "valid_bbox_ious = iou.calculate_iou(input_boxes, gt_boxes)\n",
    "print(\"IoU Matrix:\")\n",
    "print(valid_bbox_ious)\n",
    "\n",
    "# Flatten the IoU matrix and create a DataFrame\n",
    "num_input_boxes = input_boxes.shape[0]\n",
    "num_gt_boxes = gt_boxes.shape[0]\n",
    "data = []\n",
    "for i in range(num_input_boxes):\n",
    "    for j in range(num_gt_boxes):\n",
    "        data.append([i, j, valid_bbox_ious[i, j].item()])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"input_box_index\", \"gt_box_index\", \"iou\"])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"iou_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign positive label if:\n",
    "    - Find highest IoU value with each gt, and all the anchor bboxes that yields the highest IoU\n",
    "      - Here it might happen that more than 2 anchors come for 1 gt, and this is fine,\n",
    "        as this is highest IoU value from perspective of GT\n",
    "    - Find each anchor boxes, highest IoU giving GT\n",
    "      - From these indexes select only the one's having IoU > 0.7 threshold\n",
    "      - The reason for doing this is to minimize the search space.\n",
    "      - If we directly use threshold, then it can happen that more than one anchor box is available to 1 gt,\n",
    "        and we would want to avoid this scenario from perspective of Anchor Box\n",
    "  - Assign negative label if:\n",
    "    - IoU is lower than 0.3 for **all gt's**\n",
    "  - If not positive, nor -ve then ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case-I Finding argmax iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Simply doing argmax and collecting that anchor might not be enough because argmax only gives first occurence of highest element as in above example\n",
    "\n",
    "# index of anchors having highest iou's with gt's\n",
    "gt_argmax_ious = valid_bbox_ious.argmax(axis=0)  # Column wise\n",
    "# iou values at these argmax index\n",
    "gt_max_ious = valid_bbox_ious[\n",
    "    gt_argmax_ious, [0, 1]\n",
    "]  # Select 0th column in first row and 1st column in the second row\n",
    "\n",
    "\n",
    "# get all the indexes of anchors having these high value of iou's with gt\n",
    "gt_max_iou_anchors_index = np.where(valid_bbox_ious == gt_max_ious)[0]\n",
    "# (array([3331, 8535]), array([0, 1]))\n",
    "# This means element at (3331,0) and (8535,1) correspond to the gt_max_ious\n",
    "# If there could have been more anchors yielding the same highest gt_max_iou\n",
    "# values, then we could have more than 2 indices, i.e. more than 2 anchors\n",
    "\n",
    "print(gt_argmax_ious, gt_max_ious, gt_max_iou_anchors_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case-II: Finding GT for which Anchor yields a high value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of Gt's for which an anchor gives high IoU as compare to other Gt's\n",
    "anchor_argmax_ious = valid_bbox_ious.argmax(axis=1)\n",
    "\n",
    "# Getting the max iou values\n",
    "anchor_max_ious = valid_bbox_ious[\n",
    "    np.arange(len(valid_boxes)), anchor_argmax_ious\n",
    "]\n",
    "\n",
    "\n",
    "print(anchor_argmax_ious.shape, anchor_max_ious.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3\n",
    "# - Assign negative label (0) to all the anchor boxes which have max_iou less than negative threshold\n",
    "# NOTE: Finding the max of 2 anchors, decreased the search space as now we only need to compare the max anchors\n",
    "valid_labels[anchor_max_ious < neg_iou_threshold] = 0\n",
    "# Assign positive label (1) to all the anchor boxes which have Highest IoU overlap with gt-bbox\n",
    "# Assign positive label (1) to all the anchor boxes which have max_iou greater than pos_threshold\n",
    "valid_labels[gt_argmax_ious] = 1\n",
    "valid_labels[anchor_max_ious >= pos_iou_threshold] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta = np.where(valid_labels == -1)\n",
    "print(f\"We will ignore these many samples from RPN {nta[0].shape} training\")\n",
    "\n",
    "\n",
    "nta = np.where((valid_labels == 0))\n",
    "print(\n",
    "    f\"We will include these many -ve samples from RPN {nta[0].shape} training\"\n",
    ")\n",
    "nta = np.where((valid_labels == 1))\n",
    "print(\n",
    "    f\"We will include these many +ve samples from RPN {nta[0].shape} training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating Target Anchor Bboxes for Training RPN\n",
    "\n",
    "- We are sampling from the above filtered bboxes which are near to the GT's and chosen from all bboxes above which generated +ve and -ve IoU value with GT\n",
    "- Once we have labels, we only use some of them for training RPN\n",
    "- Each mini-batch arises from a single image that contains many positive and negitive example anchor bboxes, but this will bias towards negitive samples as they dominate\n",
    "- Instead, we randomly sample 256 anchor bboxes in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1\n",
    "- If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio = 0.5\n",
    "n_sample = 256\n",
    "\n",
    "\n",
    "# Total Positive Samples\n",
    "n_pos = pos_ratio * n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ramdomly sample n_pos samples from the positive labels and ignore(-1) the remaining ones.\n",
    "- If the pos samples are less than 128 we fill the remaining ones with -ve samples (0) and ignore the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg = n_sample * np.sum(valid_labels == 1)\n",
    "n_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = np.where(valid_labels == 1)[0]\n",
    "if len(pos_index) > n_pos:\n",
    "    # Randomly sample indices to not use in the training\n",
    "    disable_index = np.random.choice(\n",
    "        pos_index, size=(len(pos_index) - n_pos), replace=False\n",
    "    )\n",
    "    valid_labels[disable_index] = -1\n",
    "    # Negative Samples\n",
    "n_neg = n_sample * np.sum(valid_labels == 1)\n",
    "neg_index = np.where(valid_labels == 0)[0]\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(\n",
    "        neg_index, size=(len(neg_index) - n_neg), replace=False\n",
    "    )\n",
    "    valid_labels[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta = np.where(valid_labels == -1)\n",
    "print(f\"We will ignore these many samples from RPN {nta[0].shape} training\")\n",
    "\n",
    "\n",
    "nta = np.where((valid_labels == 0))\n",
    "print(\n",
    "    f\"We will include these many -ve samples from RPN {nta[0].shape} training\"\n",
    ")\n",
    "nta = np.where((valid_labels == 1))\n",
    "print(\n",
    "    f\"We will include these many +ve samples from RPN {nta[0].shape} training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the difference between anchor bboxes and Gt's\n",
    "- Here we find the differneces between sampled anchor boxes and Gt's and make the RPN learn these diferences\n",
    "- We select which ever gt has high iou with that anchor box.\n",
    "- NOTE: We will assign anchor locs to all valid anchor boxes, irrespective of its label.\n",
    "- Later when we calculate the losses, we can remove the irrelevant anchor boxes.\n",
    "Thes differences are calculated as follows:\n",
    "    - $t_{x} = (x-x_{a})/w_{a}$\n",
    "    - $t_{y} = (y - y_{a})/h_{a}$\n",
    "    - $t_{w} = log(w/ w_{a})$\n",
    "    - $t_{h} = log(h/ h_{a})$\n",
    "    - $x,y,w,h are the gt box center coordinates, width and height\n",
    "    - $x_{a},y_{a},w_{a},h_{a}$ are represent centre coordinates and width and height of anchor boxes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_argmax_ious.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each anchor box we select the gt that has max iou with the anchor box\n",
    "# The elements will be 8940 itself, we just return the index of gt that has highest overlap with that anchor box\n",
    "max_iou_bbox = gt_bboxes[anchor_argmax_ious]\n",
    "print(max_iou_bbox.shape)\n",
    "print(max_iou_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE; Before we proceed further we need to convert xyxy bbox format of our anchor boxes and gt boxes to xywh.\n",
    "height = valid_boxes[:, 2] - valid_boxes[:, 0]\n",
    "width = valid_boxes[:, 3] - valid_boxes[:, 1]\n",
    "ctr_y = valid_boxes[:, 0] + 0.5 * height\n",
    "ctr_x = valid_boxes[:, 1] + 0.5 * width\n",
    "\n",
    "\n",
    "base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Anchor Locations with above formulaes\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(anchor_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the locations and labels to the whole Anchors (22500) and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Labels\n",
    "anchor_labels = np.empty((len(anchor_bboxes),), dtype=valid_labels.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[valid_indices] = valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta = np.where(anchor_labels == -1)\n",
    "print(f\"We will ignore these many samples from RPN {nta[0].shape} training\")\n",
    "\n",
    "\n",
    "nta = np.where((anchor_labels == 0))\n",
    "print(\n",
    "    f\"We will include these many -ve samples from RPN {nta[0].shape} training\"\n",
    ")\n",
    "nta = np.where((anchor_labels == 1))\n",
    "print(\n",
    "    f\"We will include these many +ve samples from RPN {nta[0].shape} training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Anchors\n",
    "anchor_locations = np.empty(\n",
    "    (len(anchor_bboxes),) + anchor_bboxes.shape[1:], dtype=anchor_locs.dtype\n",
    ")\n",
    "anchor_locations.fill(0)\n",
    "# Assigning the filtered anchor bboxes, with the translated bboxes\n",
    "anchor_locations[valid_indices, :] = anchor_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anchor_labels.shape, anchor_locations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **These labels and Anchors (with valid labels) will be used as targets to the RPN network.**\n",
    "\n",
    "## 6. Region Proposal Network\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*rQ99lLIs7xTAjTaKHHNatA.png\" alt=\"Image\" style=\"height:300px; width:600px; align:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prior to this work, region proposals for a network were generated using selective search, CPMC, MCG, Edgeboxes etc. Faster_R-CNN is the first work to demonstrate generating region proposals using deep learning.\n",
    "- The network contains a convolution module, on top of which there will be one regression layer, **which predicts the location of the box inside the anchor**\n",
    "- To generate region proposals, we slide a small network over the feature map op.\n",
    "  - This small network takes as input an nxn spatial window of the ip feature map\n",
    "  - Each sliding window is mapped to a lower dimensional feature (Basically conv operation)\n",
    "  - The op of the small conv network is fed to 2 FC layers:\n",
    "    - A box regression layer\n",
    "    - A box classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use n = 3, as Faster-RCNN paper. We can any other value of n as well.\n",
    "mid_channels = 512\n",
    "in_channels = 512  # NOTE: Dependent on the feature map given by the backbone, for VGG16 it's 512\n",
    "n_anchor_boxes = 9  # Number of anchor boxes at each anchor point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(device)\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor_boxes * 4, 1, 1, 0).to(device)\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor_boxes * 2, 1, 1, 0).to(\n",
    "    device\n",
    ")  ## I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1.\n",
    "\n",
    "\n",
    "# Initial Initialization as per the paper\n",
    "# conv sliding layer\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "# Regression layer\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "# classification layer\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()\n",
    "\n",
    "# Sending the ops from feature extraction state to this network to predict location of objects wrt anchor and the objectness score associated with it\n",
    "x = conv1(feature_map)  # Feature Map Obtained in Sec 3\n",
    "pred_anchor_locs = reg_layer(x)\n",
    "# NOTE Anchor locs are the anchor bboxes and Not  the translated locations.\n",
    "# So the network will try to learn such anchor bboxes, who are very similar to the translated anchor bboxes we generated above.\n",
    "# N these predicted anchor bboxes will have ideally max ious with GT.\n",
    "pred_cls_scores = cls_layer(x)\n",
    "print(pred_cls_scores.shape, pred_anchor_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting the outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_anchor_locs = (\n",
    "    pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    ")\n",
    "print(pred_anchor_locs.shape)\n",
    "# [1, 36(9*4), 50, 50] => [1, 22500(50*50*9), 4] (dy, dx, dh, dw)\n",
    "# Out: torch.Size([1, 22500, 4])\n",
    "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "print(pred_cls_scores.shape)\n",
    "# Out torch.Size([1, 50, 50, 18])\n",
    "# [1, 18(9*2), 50, 50] => [1, 22500, 2]  (1, 0)\n",
    "objectness_score = (\n",
    "    pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1]\n",
    "    .contiguous()\n",
    "    .view(1, -1)\n",
    ")\n",
    "print(objectness_score.shape)\n",
    "# Out torch.Size([1, 22500])\n",
    "pred_cls_scores = pred_cls_scores.view(1, -1, 2)\n",
    "print(pred_cls_scores.shape)\n",
    "# Out torch.size([1, 22500, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pred_cls_scores and pred_anchor_locs are the output the RPN network and are used to calculate the losses to updates the weights of the network\n",
    "- pred_cls_scores(labels) and objectness_scores(coordinates) are used as inputs to the proposal layer, which generate a set of proposal which are further used by RoI network. We will see this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_anchor_locs.shape)\n",
    "print(pred_cls_scores.shape)\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_loc = pred_anchor_locs[0]\n",
    "rpn_score = pred_cls_scores[0]\n",
    "\n",
    "gt_rpn_loc = torch.from_numpy(anchor_locations)\n",
    "gt_rpn_score = torch.from_numpy(anchor_labels)\n",
    "\n",
    "print(rpn_loc.shape, rpn_score.shape, gt_rpn_loc.shape, gt_rpn_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification we use cross-entropy loss\n",
    "rpn_cls_loss = nn.functional.cross_entropy(\n",
    "    rpn_score, gt_rpn_score.long().to(device), ignore_index=-1\n",
    ")\n",
    "print(rpn_cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Regression we use smooth L1 loss as defined in the Fast RCNN paper\n",
    "pos = gt_rpn_score > 0  # Considering positive samples\n",
    "mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes which have positve labels\n",
    "mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x - 0.5))\n",
    "print(rpn_loc_loss.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Proposals from RPN to feed Fast R-CNN network\n",
    "\n",
    "- The proposal function will take the following parameters\n",
    "  - training_mode or testing_mode\n",
    "  - nms_thresh\n",
    "  - n_train_pre_nms: Number of bboxes before nms during training\n",
    "  - n_train_post_nms: Number of bboxes after nms during training\n",
    "  - n_test_pre_nms: Number of bboxes before nms during testing\n",
    "  - n_test_post_nms: Number of bboxes after nms during testing\n",
    "  - min_size: Minimum height of the object required to create a proposal\n",
    "\n",
    "- The Faster R_CNN says, RPN proposals highly overlap with each other.\n",
    "- To reduced redundancy, we adopt non-maximum supression (NMS) on the proposal regions based on their cls scores.\n",
    "- We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image.\n",
    "- After an ablation study, the authors show that NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals.\n",
    "- After NMS, we use the top-N ranked proposal regions for detection. In the following we training Fast R-CNN using 2000 RPN proposals.\n",
    "- During testing they evaluate only 300 proposals, they have tested this with various numbers and obtained this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_thresh = 0.7\n",
    "n_train_pre_nms = 12000\n",
    "n_train_post_nms = 2000\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300\n",
    "min_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To generate region proposals to the network following steps will be followed\n",
    "  - Convert the loc (anchor location) predictions from the rpn network to bbox [y1,x1,y2,x2]\n",
    "  - Clip the predicted boxes to the image\n",
    "  - Remove predicted boxes with either height or width < threshold (minsize)\n",
    "  - Sort all (proposal, score) pairs by score from highest to lowest\n",
    "  - Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
    "  - Apply nms threshold > 0.7\n",
    "  - Take top pos_nms_topN (eg. 2000 while training and 300 while testing)\n",
    "\n",
    "### Converting the Anchor Location Predictions to BBox\n",
    "\n",
    "- This operation is reverse operation of what we did while assigning gt to anchor boxes.\n",
    "- This operation decodes predictions by unparameterizing them and offseting to image.\n",
    "  - #ctr is for center\n",
    "  - $x = (w_{a} * ctr_{xp}) + ctr_{xa}$\n",
    "  - y = $(h_{a} * ctr_{xp}) + ctr_{xa}$\n",
    "  - $h = np.exp(h_{p}) * h_{a}$\n",
    "  - $w = np.exp(w_{p}) * w_{a}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert anchors bbox format from y1, x1, y2, x2 to ctr_x, ctr_y, h,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_height = anchor_bboxes[:, 2] - anchor_bboxes[:, 0]\n",
    "anc_width = anchor_bboxes[:, 3] - anchor_bboxes[:, 1]\n",
    "anc_ctr_y = anchor_bboxes[:, 0] + 0.5 * anc_height\n",
    "anc_ctr_x = anchor_bboxes[:, 1] + 0.5 * anc_width\n",
    "print(anc_ctr_x.shape, anc_ctr_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert prediction locations using above formulas, before doing this don't forget to get data in np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_anchor_locs_numpy = pred_anchor_locs[0].cpu().data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].cpu().data.numpy()\n",
    "dy = pred_anchor_locs_numpy[:, 0::4]\n",
    "dx = pred_anchor_locs_numpy[:, 1::4]\n",
    "dh = pred_anchor_locs_numpy[:, 2::4]\n",
    "dw = pred_anchor_locs_numpy[:, 3::4]\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]\n",
    "\n",
    "print(dy.shape, w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert [ctr_x, ctr_y, h, w] to [y1, x1, y2, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=anchor_locs.dtype)\n",
    "roi[:, 0::4] = ctr_y - 0.5 * h\n",
    "roi[:, 1::4] = ctr_x - 0.5 * w\n",
    "roi[:, 2::4] = ctr_y + 0.5 * h\n",
    "roi[:, 3::4] = ctr_x + 0.5 * w\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP the predicted boxes to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (800, 800)  # Image size\n",
    "# Clip the y coordinates to 0 to 800\n",
    "roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[1])\n",
    "# Clip the x coordinates to 0 to 800\n",
    "roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[0])\n",
    "print(roi.shape, np.max(roi), np.min(roi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove predicted boxes with either height or width < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = roi[:, 2] - roi[:, 0]\n",
    "ws = roi[:, 3] - roi[:, 1]\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "roi = roi[keep, :]\n",
    "score = objectness_score_numpy[keep]\n",
    "print(keep.shape, roi.shape, score.shape)\n",
    "# Out:\n",
    "##(22500, ) all the boxes have minimum size of 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort all (proposal, score) pairs by the score from highest to lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = score.ravel().argsort()[::-1]\n",
    "print(order.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the top pre_nms_topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
    "order = order[:n_train_pre_nms]\n",
    "roi = roi[order, :]\n",
    "print(roi.shape)\n",
    "print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply NMS\n",
    "- Overlapping bboxes are merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = roi[:, 0]\n",
    "x1 = roi[:, 1]\n",
    "y2 = roi[:, 2]\n",
    "x2 = roi[:, 3]\n",
    "\n",
    "areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "order = order.argsort()[::-1]\n",
    "keep = []\n",
    "while order.size > 0:\n",
    "    i = order[0]  # take the 1st elementt in order and append to keep\n",
    "    keep.append(i)\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "    inter = w * h\n",
    "    ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "    inds = np.where(ovr <= nms_thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "\n",
    "keep = keep[:n_train_post_nms]  # while training/testing , use accordingly\n",
    "roi = roi[keep]  # the final region proposals\n",
    "print(len(keep), roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generating Proposal Targets\n",
    "\n",
    "#### NOTE\n",
    "\n",
    "- The final region proposals were obtained.\n",
    "  - These will be used as the ip to the Fast-RCNN object which finally tries to predict the object locations (with respect to the proposesd box)\n",
    "  - Also it tries to predict the class of the object (classification of each proposal)\n",
    "- Before feeding the proposals to the Fast-RCNN:\n",
    "  - We need to first create targets for these proposals for training the network\n",
    "- Once we have both targets and proposals:\n",
    "  - We pass proposals to the network\n",
    "  - Obtain the predicted Ops\n",
    "  - Determine the losses\n",
    "\n",
    "- The Fast R-CNN network takes the region proposals (obtained from proposal layer in previous section), ground truth boxes and their respective labels as inputs.\n",
    "- It will take the following parameters:\n",
    "  - n_sample: Number of samples to sample from roi, The default value is 128.\n",
    "  - pos_ratio: the number of positive examples out of the n_samples. The default values is 0.25.\n",
    "  - pos_iou_thresh: The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\n",
    "  - [neg_iou_threshold_lo, neg_iou_threshold_hi] : [0.0, 0.5], The overlap value bounding required to consider a region proposal as negitive [background object]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 128\n",
    "pos_ratio = 0.25\n",
    "pos_iou_thresh = 0.5\n",
    "neg_iou_thresh_hi = 0.5\n",
    "neg_iou_thresh_lo = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following steps are used to generate the proposal targets\n",
    "\n",
    "  - For each roi, find the IoU with all other gt object ([N,n])\n",
    "    - N: Number of Region Proposals\n",
    "    - n: Number of gt boxes\n",
    "  - Find which gt object has highest iou with the roi [N]\n",
    "    - These gt's labels will be the labels for each and every region proposal\n",
    "  - If the highest IoU is greater than pos_iou_thresh, then we assign the label of gt\n",
    "    - Pos_Samples_Collection:\n",
    "      - We randomly sample [n_sample x pos_ratio] region proposals and consider only these as positive labels\n",
    "  - If the IoU is between [0.1, 0.5], we assign a negative label(0) to the region proposal\n",
    "\n",
    "    - Neg_Samples_Collection:\n",
    "      - We randomly sample [128 - number of pos region proposals on this image] and assign 0 to these region proposals\n",
    "\n",
    "  - We collect the pos samples and neg_samples and remove all other region proposals\n",
    "  - Lastly we convert the locations of gt objects for each region proposal to the required format (As required by Fast R-CNN)\n",
    "  - Output labels and locations for the samples ROIs (i.e. the targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_tensor = tv_tensors.BoundingBoxes(\n",
    "    roi, format=\"XYXY\", canvas_size=v2.functional.get_size(image)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the IoU of gt_object with region proposals\n",
    "target_region_proposal_ious = iou.calculate_iou(\n",
    "    roi_tensor.data, gt_bboxes.data\n",
    ")\n",
    "print(target_region_proposal_ious.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU\n",
    "gt_assignment = target_region_proposal_ious.argmax(axis=1)\n",
    "max_iou = target_region_proposal_ious.max(axis=1)\n",
    "print(gt_assignment)\n",
    "print(max_iou)\n",
    "\n",
    "# Assign the labels to each proposal\n",
    "gt_labels = np.array(gt_labels)\n",
    "gt_roi_label = gt_labels[gt_assignment]\n",
    "print(gt_roi_label)\n",
    "\n",
    "# Note: Incase if u have not taken the background object as 0, add +1 to all the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the foreground rois as per the pos_iou_thesh and\n",
    "# n_sample x pos_ratio (128 x 0.25 = 32) foreground samples.\n",
    "pos_roi_per_image = 32\n",
    "pos_index = np.where(max_iou.values >= pos_iou_thresh)[0]\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "if pos_index.size > 0:\n",
    "    pos_index = np.random.choice(\n",
    "        pos_index, size=pos_roi_per_this_image, replace=False\n",
    "    )\n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)\n",
    "\n",
    "# Similarly we do for negitive (background) region proposals\n",
    "neg_index = np.where(\n",
    "    (max_iou.values < neg_iou_thresh_hi)\n",
    "    & (max_iou.values >= neg_iou_thresh_lo)\n",
    ")[0]\n",
    "neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "if neg_index.size > 0:\n",
    "    neg_index = np.random.choice(\n",
    "        neg_index, size=neg_roi_per_this_image, replace=False\n",
    "    )\n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying +ve and -ve samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_imgs = []\n",
    "roi_bboxes = []\n",
    "pos_labels = []\n",
    "for i in range(pos_roi_per_this_image):\n",
    "    roi_bbox = roi[pos_index[i]].astype(int)\n",
    "    roi_bboxes.append(roi_bbox)\n",
    "    pos_labels.append(gt_roi_label[pos_index[i]].astype(str))\n",
    "\n",
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    roi_bboxes, format=\"XYXY\", canvas_size=v2.functional.get_size(image)\n",
    ")\n",
    "colors = [\n",
    "    mcolors.rgb2hex(plt.get_cmap(\"tab20\")(i)) for i in range(len(torch_bboxes))\n",
    "]\n",
    "font = \"UbuntuMono-R.ttf\"\n",
    "positive_samples_bbox_img = draw_bounding_boxes(\n",
    "    image,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    labels=pos_labels,\n",
    "    colors=colors,\n",
    ")\n",
    "\n",
    "roi_imgs = []\n",
    "roi_bboxes = []\n",
    "neg_label = []\n",
    "for i in range(neg_roi_per_this_image):\n",
    "    roi_bbox = roi[neg_index[i]].astype(int)\n",
    "    roi_bboxes.append(roi_bbox)\n",
    "    neg_label.append(gt_roi_label[neg_index[i]].astype(str))\n",
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    roi_bboxes, format=\"XYXY\", canvas_size=v2.functional.get_size(image)\n",
    ")\n",
    "colors = [\n",
    "    mcolors.rgb2hex(plt.get_cmap(\"tab20\")(i)) for i in range(len(torch_bboxes))\n",
    "]\n",
    "font = \"UbuntuMono-R.ttf\"\n",
    "negative_samples_bbox_img = draw_bounding_boxes(\n",
    "    image,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    labels=neg_label,\n",
    "    colors=colors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv.plot(\n",
    "    [positive_samples_bbox_img, negative_samples_bbox_img],\n",
    "    subplots_kwargs={\"figsize\": (10, 10)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we gather positive sample index and negative sample index, their respective labels and region proposals\n",
    "keep_index = np.append(pos_index, neg_index)\n",
    "gt_roi_labels = gt_roi_label[keep_index]\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
    "sample_roi = roi[keep_index]\n",
    "print(sample_roi.shape)\n",
    "\n",
    "\n",
    "# Pick the gt objects for these sample_roi and later parameterize as we have done while assigning locations to anchor boxes in section 5.\n",
    "bbox_for_sampled_roi = gt_bboxes[gt_assignment[keep_index]]\n",
    "print(bbox_for_sampled_roi.shape)\n",
    "\n",
    "height = sample_roi[:, 2] - sample_roi[:, 0]\n",
    "width = sample_roi[:, 3] - sample_roi[:, 1]\n",
    "ctr_y = sample_roi[:, 0] + 0.5 * height\n",
    "ctr_x = sample_roi[:, 1] + 0.5 * width\n",
    "\n",
    "base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
    "base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
    "base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Formulation\n",
    "\n",
    "- $t_{x} = (x - x_{a})/w_{a}$\n",
    "- $t_{y} = (y - y_{a})/h_{a}$\n",
    "- $t_{w} = log(w/ w_a)$\n",
    "- $t_{h} = log(h/ h_a)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(gt_roi_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So now we have gt_roi_locs, and gt_roi_labels for the sampled rois.\n",
    "- We now need to design the Fast RCNN network and predict the locs and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fast R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = torch.from_numpy(sample_roi).float()\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
    "roi_indices = torch.from_numpy(roi_indices).float()\n",
    "print(rois.shape, roi_indices.shape)\n",
    "\n",
    "indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "indices_and_rois = xy_indices_and_rois.contiguous()\n",
    "print(xy_indices_and_rois.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "rois = indices_and_rois.data.float()\n",
    "rois[:, 1:].mul_(1 / 16.0)  # Subsampling ratio\n",
    "rois = rois.long()\n",
    "num_rois = rois.size(0)\n",
    "for i in range(num_rois):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = feature_map.narrow(0, im_idx, 1)[\n",
    "        ..., roi[2] : (roi[4] + 1), roi[1] : (roi[3] + 1)\n",
    "    ]\n",
    "    tmp = adaptive_max_pool(im)\n",
    "    output.append(tmp[0])\n",
    "output = torch.cat(output, 0)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 5 ROI's feature map (for each feature map, only show the 1st channel of d=512)\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "for i in range(5):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = feature_map.narrow(0, im_idx, 1)[\n",
    "        ..., roi[2] : (roi[4] + 1), roi[1] : (roi[3] + 1)\n",
    "    ]\n",
    "    tmp = im[0][0].detach().cpu().numpy()\n",
    "    fig.add_subplot(1, 5, figNo)\n",
    "    plt.imshow(tmp, cmap=\"gray\")\n",
    "    figNo += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the tensor so that we can pass it through the feed forward layer.\n",
    "k = output.view(output.size(0), -1)\n",
    "print(k.shape)  # 25088 = 7*7*512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the bboxes & Features of 128 ROI samples to Detection Network to predict bbox and class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_head_classifier = nn.Sequential(\n",
    "    *[nn.Linear(25088, 4096), nn.Linear(4096, 4096)]\n",
    ").to(device)\n",
    "cls_loc = nn.Linear(4096, 2 * 4).to(\n",
    "    device\n",
    ")  # (1 classes + 1 background. Each will have 4 co-ordinates)\n",
    "cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_loc.bias.data.zero_()\n",
    "\n",
    "score = nn.Linear(4096, 2).to(device)  # (1 classes + 1 background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the output of roi-pooling to ROI head\n",
    "k = roi_head_classifier(k.to(device))\n",
    "roi_cls_loc = cls_loc(k)\n",
    "roi_cls_score = score(k)\n",
    "print(roi_cls_loc.shape, roi_cls_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FastRCNN loss based on the gt bboxes and features of the 128 ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted\n",
    "print(roi_cls_loc.shape)\n",
    "print(roi_cls_score.shape)\n",
    "\n",
    "# actual\n",
    "print(gt_roi_locs.shape)\n",
    "print(gt_roi_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting ground truth to torch variable\n",
    "gt_roi_loc = torch.from_numpy(gt_roi_locs)\n",
    "gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
    "print(gt_roi_loc.shape, gt_roi_label.shape)\n",
    "\n",
    "# Classification loss\n",
    "roi_cls_loss = nn.functional.cross_entropy(\n",
    "    roi_cls_score.cpu(), gt_roi_label.cpu(), ignore_index=-1\n",
    ")\n",
    "print(roi_cls_loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression loss\n",
    "n_sample = roi_cls_loc.shape[0]\n",
    "roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "print(roi_loc.shape)\n",
    "\n",
    "roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "print(roi_loc.shape)\n",
    "\n",
    "# For Regression we use smooth L1 loss as defined in the Fast RCNN paper\n",
    "pos = gt_roi_label > 0\n",
    "mask = pos.unsqueeze(1).expand_as(roi_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes which have positve labels\n",
    "mask_loc_preds = roi_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_roi_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "roi_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x - 0.5))\n",
    "print(roi_loc_loss.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_lambda = 10.0\n",
    "roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)\n",
    "print(roi_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = rpn_loss.to(device) + roi_loss.to(device)\n",
    "print(total_loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask_loc_preds.shape, pos.shape, mask_loc_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gt_roi_label[gt_roi_label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
